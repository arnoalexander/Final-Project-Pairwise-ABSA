{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pairing import Reader, Extractor, BaselineClassifier, GBClassifier, FilteredGBClassifier\n",
    "import definition\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': ['kamar',\n",
       "  'saya',\n",
       "  'ada',\n",
       "  'kendala',\n",
       "  'di',\n",
       "  'ac',\n",
       "  'tidak',\n",
       "  'berfungsi',\n",
       "  'optimal',\n",
       "  '.',\n",
       "  'dan',\n",
       "  'juga',\n",
       "  'wifi',\n",
       "  'koneksi',\n",
       "  'kurang',\n",
       "  'stabil',\n",
       "  '.'],\n",
       " 'label': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ASPECT',\n",
       "  'B-SENTIMENT',\n",
       "  'I-SENTIMENT',\n",
       "  'I-SENTIMENT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ASPECT',\n",
       "  'I-ASPECT',\n",
       "  'B-SENTIMENT',\n",
       "  'I-SENTIMENT',\n",
       "  'O'],\n",
       " 'aspect': [{'start': 5, 'length': 1}, {'start': 12, 'length': 2}],\n",
       " 'sentiment': [{'start': 6, 'length': 3, 'index_aspect': [0]},\n",
       "  {'start': 14, 'length': 2, 'index_aspect': [1]}]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = Reader.read_file(definition.DATA_PAIRED_TRAIN)\n",
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor definition\n",
    "\n",
    "embedding_filename = \"word2vec_50.bin\"\n",
    "word_count_filename = \"word_count_60.pkl\"\n",
    "clustering_filename = \"word2vec_50_kmeans_50.pkl\"\n",
    "\n",
    "extractor = Extractor(\n",
    "    embedding_filename=os.path.join(definition.MODEL_UTILITY, embedding_filename), \n",
    "    word_count_filename=os.path.join(definition.MODEL_UTILITY, word_count_filename),\n",
    "    clustering_filename=os.path.join(definition.MODEL_UTILITY, clustering_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|█████████████████████████████████████████████████████████████| 4000/4000 [01:05<00:00, 61.20it/s]\n"
     ]
    }
   ],
   "source": [
    "data = extractor.extract_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|█████████████████████████████████████████████████████████████| 4000/4000 [00:51<00:00, 78.05it/s]\n"
     ]
    }
   ],
   "source": [
    "data_chen = extractor.extract_data(raw_data, include_new_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL\t[+]\t[-]\n",
      "27894\t8748\t19146\n"
     ]
    }
   ],
   "source": [
    "print('TOTAL', '[+]', '[-]', sep='\\t')\n",
    "print(len(data), len(data[data['target']==1]), len(data[data['target']==0]), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_chen = data_chen.drop('target', axis=1)\n",
    "y_chen = data_chen['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id_aspect</th>\n",
       "      <th>_id_closest_sentiment</th>\n",
       "      <th>_id_sentence</th>\n",
       "      <th>_id_sentiment</th>\n",
       "      <th>_n_aspect</th>\n",
       "      <th>_n_sentiment</th>\n",
       "      <th>cos_aspect_sentence</th>\n",
       "      <th>cos_aspect_sentiment</th>\n",
       "      <th>cos_aspect_sentiment_validity</th>\n",
       "      <th>cos_sentiment_sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>v_sentiment_45</th>\n",
       "      <th>v_sentiment_46</th>\n",
       "      <th>v_sentiment_47</th>\n",
       "      <th>v_sentiment_48</th>\n",
       "      <th>v_sentiment_49</th>\n",
       "      <th>v_sentiment_5</th>\n",
       "      <th>v_sentiment_6</th>\n",
       "      <th>v_sentiment_7</th>\n",
       "      <th>v_sentiment_8</th>\n",
       "      <th>v_sentiment_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "      <td>27894.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.581200</td>\n",
       "      <td>1.909550</td>\n",
       "      <td>1979.759984</td>\n",
       "      <td>1.953646</td>\n",
       "      <td>4.162401</td>\n",
       "      <td>4.907292</td>\n",
       "      <td>0.415881</td>\n",
       "      <td>0.184183</td>\n",
       "      <td>0.974331</td>\n",
       "      <td>0.429686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093748</td>\n",
       "      <td>1.590655</td>\n",
       "      <td>1.076279</td>\n",
       "      <td>-1.484029</td>\n",
       "      <td>-0.227094</td>\n",
       "      <td>0.478480</td>\n",
       "      <td>-0.337746</td>\n",
       "      <td>-0.025838</td>\n",
       "      <td>0.112124</td>\n",
       "      <td>0.237303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.793977</td>\n",
       "      <td>1.995934</td>\n",
       "      <td>1148.287233</td>\n",
       "      <td>2.013907</td>\n",
       "      <td>2.360831</td>\n",
       "      <td>2.529206</td>\n",
       "      <td>0.209494</td>\n",
       "      <td>0.181290</td>\n",
       "      <td>0.158147</td>\n",
       "      <td>0.203547</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221933</td>\n",
       "      <td>1.471443</td>\n",
       "      <td>1.269084</td>\n",
       "      <td>1.870378</td>\n",
       "      <td>1.474619</td>\n",
       "      <td>1.857541</td>\n",
       "      <td>1.110074</td>\n",
       "      <td>0.978354</td>\n",
       "      <td>1.938403</td>\n",
       "      <td>1.325352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.535566</td>\n",
       "      <td>-0.583915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.566650</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.105083</td>\n",
       "      <td>-3.917495</td>\n",
       "      <td>-4.036607</td>\n",
       "      <td>-5.882783</td>\n",
       "      <td>-5.143031</td>\n",
       "      <td>-3.758305</td>\n",
       "      <td>-3.668407</td>\n",
       "      <td>-3.021578</td>\n",
       "      <td>-4.713010</td>\n",
       "      <td>-4.698528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.287930</td>\n",
       "      <td>0.066104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626339</td>\n",
       "      <td>0.537649</td>\n",
       "      <td>0.074207</td>\n",
       "      <td>-3.030760</td>\n",
       "      <td>-0.957772</td>\n",
       "      <td>-0.625553</td>\n",
       "      <td>-1.099299</td>\n",
       "      <td>-0.695181</td>\n",
       "      <td>-1.519508</td>\n",
       "      <td>-0.548453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.445440</td>\n",
       "      <td>0.188263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310343</td>\n",
       "      <td>1.690161</td>\n",
       "      <td>1.065387</td>\n",
       "      <td>-1.549783</td>\n",
       "      <td>0.125720</td>\n",
       "      <td>0.747519</td>\n",
       "      <td>-0.076078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2963.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.572661</td>\n",
       "      <td>0.303586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626780</td>\n",
       "      <td>2.547805</td>\n",
       "      <td>2.066790</td>\n",
       "      <td>-0.059055</td>\n",
       "      <td>0.663761</td>\n",
       "      <td>1.665720</td>\n",
       "      <td>0.527881</td>\n",
       "      <td>0.481413</td>\n",
       "      <td>1.743133</td>\n",
       "      <td>1.122411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3999.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.976030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994282</td>\n",
       "      <td>...</td>\n",
       "      <td>5.882351</td>\n",
       "      <td>4.597618</td>\n",
       "      <td>5.920994</td>\n",
       "      <td>3.350471</td>\n",
       "      <td>5.089591</td>\n",
       "      <td>3.925813</td>\n",
       "      <td>4.375823</td>\n",
       "      <td>5.992032</td>\n",
       "      <td>4.145845</td>\n",
       "      <td>4.232138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _id_aspect  _id_closest_sentiment  _id_sentence  _id_sentiment  \\\n",
       "count  27894.000000           27894.000000  27894.000000   27894.000000   \n",
       "mean       1.581200               1.909550   1979.759984       1.953646   \n",
       "std        1.793977               1.995934   1148.287233       2.013907   \n",
       "min        0.000000               0.000000      0.000000       0.000000   \n",
       "25%        0.000000               0.000000    960.000000       0.000000   \n",
       "50%        1.000000               1.000000   1994.000000       1.000000   \n",
       "75%        2.000000               3.000000   2963.000000       3.000000   \n",
       "max       11.000000              13.000000   3999.000000      13.000000   \n",
       "\n",
       "          _n_aspect  _n_sentiment  cos_aspect_sentence  cos_aspect_sentiment  \\\n",
       "count  27894.000000  27894.000000         27894.000000          27894.000000   \n",
       "mean       4.162401      4.907292             0.415881              0.184183   \n",
       "std        2.360831      2.529206             0.209494              0.181290   \n",
       "min        1.000000      1.000000            -0.535566             -0.583915   \n",
       "25%        2.000000      3.000000             0.287930              0.066104   \n",
       "50%        4.000000      4.000000             0.445440              0.188263   \n",
       "75%        5.000000      6.000000             0.572661              0.303586   \n",
       "max       12.000000     14.000000             0.976030              1.000000   \n",
       "\n",
       "       cos_aspect_sentiment_validity  cos_sentiment_sentence  ...  \\\n",
       "count                   27894.000000            27894.000000  ...   \n",
       "mean                        0.974331                0.429686  ...   \n",
       "std                         0.158147                0.203547  ...   \n",
       "min                         0.000000               -0.566650  ...   \n",
       "25%                         1.000000                0.315573  ...   \n",
       "50%                         1.000000                0.455528  ...   \n",
       "75%                         1.000000                0.576059  ...   \n",
       "max                         1.000000                0.994282  ...   \n",
       "\n",
       "       v_sentiment_45  v_sentiment_46  v_sentiment_47  v_sentiment_48  \\\n",
       "count    27894.000000    27894.000000    27894.000000    27894.000000   \n",
       "mean         0.093748        1.590655        1.076279       -1.484029   \n",
       "std          1.221933        1.471443        1.269084        1.870378   \n",
       "min         -5.105083       -3.917495       -4.036607       -5.882783   \n",
       "25%         -0.626339        0.537649        0.074207       -3.030760   \n",
       "50%          0.310343        1.690161        1.065387       -1.549783   \n",
       "75%          0.626780        2.547805        2.066790       -0.059055   \n",
       "max          5.882351        4.597618        5.920994        3.350471   \n",
       "\n",
       "       v_sentiment_49  v_sentiment_5  v_sentiment_6  v_sentiment_7  \\\n",
       "count    27894.000000   27894.000000   27894.000000   27894.000000   \n",
       "mean        -0.227094       0.478480      -0.337746      -0.025838   \n",
       "std          1.474619       1.857541       1.110074       0.978354   \n",
       "min         -5.143031      -3.758305      -3.668407      -3.021578   \n",
       "25%         -0.957772      -0.625553      -1.099299      -0.695181   \n",
       "50%          0.125720       0.747519      -0.076078       0.000000   \n",
       "75%          0.663761       1.665720       0.527881       0.481413   \n",
       "max          5.089591       3.925813       4.375823       5.992032   \n",
       "\n",
       "       v_sentiment_8  v_sentiment_9  \n",
       "count   27894.000000   27894.000000  \n",
       "mean        0.112124       0.237303  \n",
       "std         1.938403       1.325352  \n",
       "min        -4.713010      -4.698528  \n",
       "25%        -1.519508      -0.548453  \n",
       "50%         0.087564       0.338100  \n",
       "75%         1.743133       1.122411  \n",
       "max         4.145845       4.232138  \n",
       "\n",
       "[8 rows x 194 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = ['_id_sentence', '_id_aspect', '_id_sentiment', '_id_closest_sentiment', '_n_aspect', '_n_sentiment']\n",
    "\n",
    "# Set dropped labels depending on classifier model\n",
    "def drop_dummy_feature_baseline(X):\n",
    "    return X.drop(labels=['_id_sentence', '_id_aspect', '_n_aspect'], axis=1)\n",
    "\n",
    "# Set dropped labels depending on classifier model\n",
    "def drop_dummy_feature(X):\n",
    "    return X.drop(labels=['_id_sentence', '_id_aspect', '_id_sentiment', '_id_closest_sentiment', '_n_aspect'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'num_leaves': [31, 63, 127],\n",
    "              'max_depth': [6, 8, -1],\n",
    "              'learning_rate': [0.1, 0.2],\n",
    "              'n_estimators': [100, 200],\n",
    "              'min_child_samples': [10, 20, 40]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split 1/5]\n",
      "f1_0 : 0.961234071093226\n",
      "f1_1 : 0.9113224915618289\n",
      "f1_a : 0.9362782813275274\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3583           91\n",
      "true_1          198         1485\n",
      "\n",
      "[Split 2/5]\n",
      "f1_0 : 0.963664707998561\n",
      "f1_1 : 0.9128057553956834\n",
      "f1_a : 0.9382352316971222\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         4018           96\n",
      "true_1          207         1586\n",
      "\n",
      "[Split 3/5]\n",
      "f1_0 : 0.9529755579171094\n",
      "f1_1 : 0.8967327887981329\n",
      "f1_a : 0.9248541733576212\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3587          133\n",
      "true_1          221         1537\n",
      "\n",
      "[Split 4/5]\n",
      "f1_0 : 0.9586776859504132\n",
      "f1_1 : 0.9085631349782294\n",
      "f1_a : 0.9336204104643213\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3654          109\n",
      "true_1          206         1565\n",
      "\n",
      "[Split 5/5]\n",
      "f1_0 : 0.9602649006622516\n",
      "f1_1 : 0.9078014184397163\n",
      "f1_a : 0.9340331595509839\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3770          105\n",
      "true_1          207         1536\n",
      "\n",
      "[Summary]\n",
      "f1_0 : 0.9593633847243123\n",
      "f1_1 : 0.9074451178347183\n",
      "f1_a : 0.9334042512795154\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation (Project)\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "available_sentence_id = pd.unique(X['_id_sentence'])\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "split_idx = 1\n",
    "f1_0_total = 0.0\n",
    "f1_1_total = 0.0\n",
    "f1_a_total = 0.0\n",
    "for train_sentence_id_index, test_sentence_id_index in kfold.split(available_sentence_id):\n",
    "    print(\"[Split {}/{}]\".format(split_idx, n_splits))\n",
    "    split_idx += 1\n",
    "    \n",
    "    train_sentence_id = available_sentence_id[train_sentence_id_index]\n",
    "    train_pointer = X['_id_sentence'].isin(train_sentence_id)\n",
    "    X_train = X[train_pointer]\n",
    "    X_test = X[np.bitwise_not(train_pointer)]\n",
    "    y_train = y[train_pointer]\n",
    "    y_test = y[np.bitwise_not(train_pointer)]\n",
    "    \n",
    "    model = FilteredGBClassifier(num_leaves=127, n_estimators=200)\n",
    "    model.fit(drop_dummy_feature(X_train), y_train)\n",
    "    pred = model.predict(drop_dummy_feature(X_test))\n",
    "    f1_0 = f1_score(y_test, pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, pred, pos_label=1)\n",
    "    f1_a = f1_score(y_test, pred, average='macro')\n",
    "    f1_0_total += f1_0\n",
    "    f1_1_total += f1_1\n",
    "    f1_a_total += f1_a\n",
    "    print(\"f1_0 : {}\".format(f1_0))\n",
    "    print(\"f1_1 : {}\".format(f1_1))\n",
    "    print(\"f1_a : {}\".format(f1_a))\n",
    "    print(\"\")\n",
    "    print(FilteredGBClassifier.generate_confusion_matrix_table(y_test, pred))\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"[Summary]\")\n",
    "print(\"f1_0 : {}\".format(f1_0_total/n_splits))\n",
    "print(\"f1_1 : {}\".format(f1_1_total/n_splits))\n",
    "print(\"f1_a : {}\".format(f1_a_total/n_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split 1/5]\n",
      "f1_0 : 0.9559505409582689\n",
      "f1_1 : 0.9003496503496503\n",
      "f1_a : 0.9281500956539597\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3711          105\n",
      "true_1          237         1545\n",
      "\n",
      "[Split 2/5]\n",
      "f1_0 : 0.9548957675300063\n",
      "f1_1 : 0.8944723618090452\n",
      "f1_a : 0.9246840646695258\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3779          117\n",
      "true_1          240         1513\n",
      "\n",
      "[Split 3/5]\n",
      "f1_0 : 0.9600312459315192\n",
      "f1_1 : 0.9096793174463077\n",
      "f1_a : 0.9348552816889135\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3687           89\n",
      "true_1          218         1546\n",
      "\n",
      "[Split 4/5]\n",
      "f1_0 : 0.9611326298918124\n",
      "f1_1 : 0.9131083905643476\n",
      "f1_a : 0.93712051022808\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3598           95\n",
      "true_1          196         1529\n",
      "\n",
      "[Split 5/5]\n",
      "f1_0 : 0.961524000501316\n",
      "f1_1 : 0.9096793174463078\n",
      "f1_a : 0.9356016589738119\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         3836          129\n",
      "true_1          178         1546\n",
      "\n",
      "[Summary]\n",
      "f1_0 : 0.9587068369625846\n",
      "f1_1 : 0.9054578075231318\n",
      "f1_a : 0.9320823222428581\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation (Chen)\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "available_sentence_id = pd.unique(X_chen['_id_sentence'])\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "split_idx = 1\n",
    "f1_0_total = 0.0\n",
    "f1_1_total = 0.0\n",
    "f1_a_total = 0.0\n",
    "for train_sentence_id_index, test_sentence_id_index in kfold.split(available_sentence_id):\n",
    "    print(\"[Split {}/{}]\".format(split_idx, n_splits))\n",
    "    split_idx += 1\n",
    "    \n",
    "    train_sentence_id = available_sentence_id[train_sentence_id_index]\n",
    "    train_pointer = X_chen['_id_sentence'].isin(train_sentence_id)\n",
    "    X_train = X_chen[train_pointer]\n",
    "    X_test = X_chen[np.bitwise_not(train_pointer)]\n",
    "    y_train = y_chen[train_pointer]\n",
    "    y_test = y_chen[np.bitwise_not(train_pointer)]\n",
    "    \n",
    "    model_chen = FilteredGBClassifier(num_leaves=127, n_estimators=200, max_depth=8)\n",
    "    model_chen.fit(drop_dummy_feature(X_train), y_train)\n",
    "    pred = model_chen.predict(drop_dummy_feature(X_test))\n",
    "    f1_0 = f1_score(y_test, pred, pos_label=0)\n",
    "    f1_1 = f1_score(y_test, pred, pos_label=1)\n",
    "    f1_a = f1_score(y_test, pred, average='macro')\n",
    "    f1_0_total += f1_0\n",
    "    f1_1_total += f1_1\n",
    "    f1_a_total += f1_a\n",
    "    print(\"f1_0 : {}\".format(f1_0))\n",
    "    print(\"f1_1 : {}\".format(f1_1))\n",
    "    print(\"f1_a : {}\".format(f1_a))\n",
    "    print(\"\")\n",
    "    print(FilteredGBClassifier.generate_confusion_matrix_table(y_test, pred))\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"[Summary]\")\n",
    "print(\"f1_0 : {}\".format(f1_0_total/n_splits))\n",
    "print(\"f1_1 : {}\".format(f1_1_total/n_splits))\n",
    "print(\"f1_a : {}\".format(f1_a_total/n_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=200, n_jobs=-1, num_leaves=127, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=8,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=200, n_jobs=-1, num_leaves=127, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_chen.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(drop_dummy_feature(X), y)\n",
    "model_pairing_filename = \"pairing_final.pkl\"\n",
    "model.save(os.path.join(definition.MODEL_PAIRING, model_pairing_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chen.fit(drop_dummy_feature(X_chen), y_chen)\n",
    "model_pairing_filename = \"pairing_final_chen.pkl\"\n",
    "model_chen.save(os.path.join(definition.MODEL_PAIRING, model_pairing_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': ['pelayanan',\n",
       "  'ramah',\n",
       "  ',',\n",
       "  'kamar',\n",
       "  'nyaman',\n",
       "  'dan',\n",
       "  'fasilitas',\n",
       "  'lengkap',\n",
       "  '.',\n",
       "  'hanya',\n",
       "  'airnya',\n",
       "  'showernya',\n",
       "  'kurang',\n",
       "  'panas',\n",
       "  '.'],\n",
       " 'label': ['B-ASPECT',\n",
       "  'B-SENTIMENT',\n",
       "  'O',\n",
       "  'B-ASPECT',\n",
       "  'B-SENTIMENT',\n",
       "  'O',\n",
       "  'B-ASPECT',\n",
       "  'B-SENTIMENT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ASPECT',\n",
       "  'I-ASPECT',\n",
       "  'B-SENTIMENT',\n",
       "  'I-SENTIMENT',\n",
       "  'O'],\n",
       " 'aspect': [{'start': 0, 'length': 1},\n",
       "  {'start': 3, 'length': 1},\n",
       "  {'start': 6, 'length': 1},\n",
       "  {'start': 10, 'length': 2}],\n",
       " 'sentiment': [{'start': 1, 'length': 1, 'index_aspect': [0]},\n",
       "  {'start': 4, 'length': 1, 'index_aspect': [1]},\n",
       "  {'start': 7, 'length': 1, 'index_aspect': [2]},\n",
       "  {'start': 12, 'length': 2, 'index_aspect': [3]}]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test = Reader.read_file(definition.DATA_PAIRED_TEST)\n",
    "raw_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:12<00:00, 78.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test = extractor.extract_data(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:12<00:00, 79.95it/s]\n"
     ]
    }
   ],
   "source": [
    "test_chen = extractor.extract_data(raw_test, include_new_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL\t[+]\t[-]\n",
      "7085\t2148\t4937\n"
     ]
    }
   ],
   "source": [
    "print('TOTAL', '[+]', '[-]', sep='\\t')\n",
    "print(len(test), len(test[test['target']==1]), len(test[test['target']==0]), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop('target', axis=1)\n",
    "y_test = test['target']\n",
    "\n",
    "X_test_chen = test_chen.drop('target', axis=1)\n",
    "y_test_chen = test_chen['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_0 : 0.9561697926949654\n",
      "f1_1 : 0.8900990099009901\n",
      "f1_a : 0.9231344012979777\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         4843           94\n",
      "true_1          350         1798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "\n",
    "model_baseline = BaselineClassifier()\n",
    "pred_test_baseline = model_baseline.predict(drop_dummy_feature_baseline(X_test))\n",
    "f1_0 = f1_score(y_test, pred_test_baseline, pos_label=0)\n",
    "f1_1 = f1_score(y_test, pred_test_baseline, pos_label=1)\n",
    "f1_a = f1_score(y_test, pred_test_baseline, average='macro')\n",
    "print(\"f1_0 : {}\".format(f1_0))\n",
    "print(\"f1_1 : {}\".format(f1_1))\n",
    "print(\"f1_a : {}\".format(f1_a))\n",
    "print(\"\")\n",
    "print(BaselineClassifier.generate_confusion_matrix_table(y_test, pred_test_baseline))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_0 : 0.9605540499849444\n",
      "f1_1 : 0.9065842643213691\n",
      "f1_a : 0.9335691571531568\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         4785          152\n",
      "true_1          241         1907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best model\n",
    "\n",
    "model_best = FilteredGBClassifier()\n",
    "model_best.load(os.path.join(definition.MODEL_PAIRING, \"pairing_final.pkl\"))\n",
    "pred_test_best = model_best.predict(drop_dummy_feature(X_test))\n",
    "f1_0 = f1_score(y_test, pred_test_best, pos_label=0)\n",
    "f1_1 = f1_score(y_test, pred_test_best, pos_label=1)\n",
    "f1_a = f1_score(y_test, pred_test_best, average='macro')\n",
    "print(\"f1_0 : {}\".format(f1_0))\n",
    "print(\"f1_1 : {}\".format(f1_1))\n",
    "print(\"f1_a : {}\".format(f1_a))\n",
    "print(\"\")\n",
    "print(FilteredGBClassifier.generate_confusion_matrix_table(y_test, pred_test_best))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_0 : 0.9612527604898614\n",
      "f1_1 : 0.9082699619771863\n",
      "f1_a : 0.9347613612335239\n",
      "\n",
      "        predicted_0  predicted_1\n",
      "true_0         4788          149\n",
      "true_1          237         1911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chen model\n",
    "\n",
    "model_chen = FilteredGBClassifier()\n",
    "model_chen.load(os.path.join(definition.MODEL_PAIRING, \"pairing_final_chen.pkl\"))\n",
    "pred_test_chen = model_chen.predict(drop_dummy_feature(X_test_chen))\n",
    "f1_0 = f1_score(y_test_chen, pred_test_chen, pos_label=0)\n",
    "f1_1 = f1_score(y_test_chen, pred_test_chen, pos_label=1)\n",
    "f1_a = f1_score(y_test_chen, pred_test_chen, average='macro')\n",
    "print(\"f1_0 : {}\".format(f1_0))\n",
    "print(\"f1_1 : {}\".format(f1_1))\n",
    "print(\"f1_a : {}\".format(f1_a))\n",
    "print(\"\")\n",
    "print(FilteredGBClassifier.generate_confusion_matrix_table(y_test_chen, pred_test_chen))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
